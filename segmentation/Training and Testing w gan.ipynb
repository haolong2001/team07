{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import datetime\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    }
   ],
   "source": [
    "train_folder_imgs = np.array(glob.glob(os.path.join(\"p2psmb128\", \"images\", \"*\")))\n",
    "train_folder_labs = np.array(glob.glob(os.path.join(\"p2psmb128\", \"labels\", \"*\")))\n",
    "\n",
    "print(len(train_folder_imgs))\n",
    "np.random.seed(1)\n",
    "val_idx = np.random.choice(len(train_folder_imgs), int(2*len(train_folder_imgs)/10), replace=False).astype(int)\n",
    "val_paths = []\n",
    "train_paths = []\n",
    "for i in range(len(train_folder_imgs)):\n",
    "    if i in val_idx:\n",
    "        val_paths.append([train_folder_imgs[i], train_folder_labs[i]])\n",
    "    else:\n",
    "        train_paths.append([train_folder_imgs[i], train_folder_labs[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307, 1229)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_paths), len(train_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WoundTransformation(im, mask, p):\n",
    "\n",
    "    # Horizontal flip\n",
    "    if np.random.rand() < p:\n",
    "        im = cv2.flip(im,1)\n",
    "        mask = cv2.flip(mask,1)\n",
    "    \n",
    "    # Vertical flip\n",
    "    if np.random.rand() < p:\n",
    "        im = cv2.flip(im,0)\n",
    "        mask = cv2.flip(mask,0)\n",
    "        \n",
    "    # Gaussian noise\n",
    "    if np.random.rand() < p: # Add Gaussian noise\n",
    "        stdv = np.random.uniform(3, 12)\n",
    "        noise = np.random.normal(0, stdv, im.shape)\n",
    "        im = np.uint8(np.round(np.clip(im + noise,0,255)))\n",
    "\n",
    "    return im, mask\n",
    "\n",
    "class WoundData(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data, transform):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Read image and mask\n",
    "        im = cv2.imread(self.data[idx][0],-1)\n",
    "        mask = cv2.imread(self.data[idx][1],0)\n",
    "        \n",
    "        if self.transform:\n",
    "            im, mask = WoundTransformation(im, mask, 0.5)\n",
    "        \n",
    "        # From np.array (HxWxC) to torch.tensor (CxHxW). From [0,255] to [0,1]\n",
    "        im = torch.from_numpy(np.float32(im/255).transpose(2,0,1))\n",
    "        mask = torch.from_numpy(np.float32(mask/255)).unsqueeze(0)\n",
    "        \n",
    "        return im, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = WoundData(train_paths, True)\n",
    "val_ds = WoundData(val_paths, False)\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=0)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1229, 307)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create segmentation model with pretrained encoder\n",
    "\n",
    "ENCODER = 'timm-regnetx_006'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = ['vein']\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    in_channels=3,\n",
    ")\n",
    "\n",
    "model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch\n",
    "ALPHA = 0.4\n",
    "BETA = 0.6\n",
    "GAMMA = 2\n",
    "\n",
    "class FocalTverskyLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(FocalTverskyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA, gamma=GAMMA):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = torch.sigmoid(inputs)       \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        #True Positives, False Positives & False Negatives\n",
    "        TP = (inputs * targets).sum()    \n",
    "        FP = ((1-targets) * inputs).sum()\n",
    "        FN = (targets * (1-inputs)).sum()\n",
    "        \n",
    "        Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n",
    "        FocalTversky = (1 - Tversky)**gamma\n",
    "                       \n",
    "        return FocalTversky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loss\n",
    "loss_func = FocalTverskyLoss()\n",
    "\n",
    "# Optimizer\n",
    "opt = torch.optim.Adam(model.parameters(), lr=4e-4)\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,mode='min',factor=0.1,patience=10,verbose=1)\n",
    "\n",
    "# freeze encoder\n",
    "#for param in model.encoder.parameters():\n",
    "#    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n",
      "1 10\n",
      "1 20\n",
      "1 30\n",
      "1 40\n",
      "1 50\n",
      "1 60\n",
      "1 70\n",
      "Validation loss decreased (inf --> 0.00516384). Saving model ...\n",
      "Epoch 1/100, lr = 4.00e-04, train loss: 0.010675, val loss: 0.00516384, \n",
      "time taken for epoch:  13.369653940200806\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "2 0\n",
      "2 10\n",
      "2 20\n",
      "2 30\n",
      "2 40\n",
      "2 50\n",
      "2 60\n",
      "2 70\n",
      "Validation loss decreased (0.00516384 --> 0.0032792). Saving model ...\n",
      "Epoch 2/100, lr = 4.00e-04, train loss: 0.00451164, val loss: 0.0032792, \n",
      "time taken for epoch:  11.019538879394531\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "3 0\n",
      "3 10\n",
      "3 20\n",
      "3 30\n",
      "3 40\n",
      "3 50\n",
      "3 60\n",
      "3 70\n",
      "Validation loss decreased (0.0032792 --> 0.00187524). Saving model ...\n",
      "Epoch 3/100, lr = 4.00e-04, train loss: 0.00268874, val loss: 0.00187524, \n",
      "time taken for epoch:  9.684211015701294\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "4 0\n",
      "4 10\n",
      "4 20\n",
      "4 30\n",
      "4 40\n",
      "4 50\n",
      "4 60\n",
      "4 70\n",
      "Validation loss decreased (0.00187524 --> 0.00128423). Saving model ...\n",
      "Epoch 4/100, lr = 4.00e-04, train loss: 0.00173319, val loss: 0.00128423, \n",
      "time taken for epoch:  9.606444597244263\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "5 0\n",
      "5 10\n",
      "5 20\n",
      "5 30\n",
      "5 40\n",
      "5 50\n",
      "5 60\n",
      "5 70\n",
      "Validation loss decreased (0.00128423 --> 0.0010267). Saving model ...\n",
      "Epoch 5/100, lr = 4.00e-04, train loss: 0.0012543, val loss: 0.0010267, \n",
      "time taken for epoch:  9.264595746994019\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "6 0\n",
      "6 10\n",
      "6 20\n",
      "6 30\n",
      "6 40\n",
      "6 50\n",
      "6 60\n",
      "6 70\n",
      "Epoch 6/100, lr = 4.00e-04, train loss: 0.000951337, val loss: 0.00127317, \n",
      "time taken for epoch:  9.409273862838745\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "7 0\n",
      "7 10\n",
      "7 20\n",
      "7 30\n",
      "7 40\n",
      "7 50\n",
      "7 60\n",
      "7 70\n",
      "Validation loss decreased (0.0010267 --> 0.000899616). Saving model ...\n",
      "Epoch 7/100, lr = 4.00e-04, train loss: 0.00106755, val loss: 0.000899616, \n",
      "time taken for epoch:  9.688354253768921\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "8 0\n",
      "8 10\n",
      "8 20\n",
      "8 30\n",
      "8 40\n",
      "8 50\n",
      "8 60\n",
      "8 70\n",
      "Validation loss decreased (0.000899616 --> 0.000868711). Saving model ...\n",
      "Epoch 8/100, lr = 4.00e-04, train loss: 0.000794277, val loss: 0.000868711, \n",
      "time taken for epoch:  10.004032373428345\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "9 0\n",
      "9 10\n",
      "9 20\n",
      "9 30\n",
      "9 40\n",
      "9 50\n",
      "9 60\n",
      "9 70\n",
      "Validation loss decreased (0.000868711 --> 0.000745783). Saving model ...\n",
      "Epoch 9/100, lr = 4.00e-04, train loss: 0.000565429, val loss: 0.000745783, \n",
      "time taken for epoch:  9.217011213302612\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "10 0\n",
      "10 10\n",
      "10 20\n",
      "10 30\n",
      "10 40\n",
      "10 50\n",
      "10 60\n",
      "10 70\n",
      "Validation loss decreased (0.000745783 --> 0.000731061). Saving model ...\n",
      "Epoch 10/100, lr = 4.00e-04, train loss: 0.000629621, val loss: 0.000731061, \n",
      "time taken for epoch:  9.316219568252563\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "11 0\n",
      "11 10\n",
      "11 20\n",
      "11 30\n",
      "11 40\n",
      "11 50\n",
      "11 60\n",
      "11 70\n",
      "Validation loss decreased (0.000731061 --> 0.000721107). Saving model ...\n",
      "Epoch 11/100, lr = 4.00e-04, train loss: 0.000527001, val loss: 0.000721107, \n",
      "time taken for epoch:  9.387821674346924\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "12 0\n",
      "12 10\n",
      "12 20\n",
      "12 30\n",
      "12 40\n",
      "12 50\n",
      "12 60\n",
      "12 70\n",
      "Validation loss decreased (0.000721107 --> 0.000622485). Saving model ...\n",
      "Epoch 12/100, lr = 4.00e-04, train loss: 0.000519565, val loss: 0.000622485, \n",
      "time taken for epoch:  9.531145095825195\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "13 0\n",
      "13 10\n",
      "13 20\n",
      "13 30\n",
      "13 40\n",
      "13 50\n",
      "13 60\n",
      "13 70\n",
      "Epoch 13/100, lr = 4.00e-04, train loss: 0.000769382, val loss: 0.000738291, \n",
      "time taken for epoch:  9.512224197387695\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "14 0\n",
      "14 10\n",
      "14 20\n",
      "14 30\n",
      "14 40\n",
      "14 50\n",
      "14 60\n",
      "14 70\n",
      "Epoch 14/100, lr = 4.00e-04, train loss: 0.000525377, val loss: 0.000708808, \n",
      "time taken for epoch:  8.997302532196045\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "15 0\n",
      "15 10\n",
      "15 20\n",
      "15 30\n",
      "15 40\n",
      "15 50\n",
      "15 60\n",
      "15 70\n",
      "Epoch 15/100, lr = 4.00e-04, train loss: 0.000415155, val loss: 0.000744304, \n",
      "time taken for epoch:  8.97039246559143\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "16 0\n",
      "16 10\n",
      "16 20\n",
      "16 30\n",
      "16 40\n",
      "16 50\n",
      "16 60\n",
      "16 70\n",
      "Epoch 16/100, lr = 4.00e-04, train loss: 0.00044363, val loss: 0.000853595, \n",
      "time taken for epoch:  9.285029411315918\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "17 0\n",
      "17 10\n",
      "17 20\n",
      "17 30\n",
      "17 40\n",
      "17 50\n",
      "17 60\n",
      "17 70\n",
      "Validation loss decreased (0.000622485 --> 0.000577215). Saving model ...\n",
      "Epoch 17/100, lr = 4.00e-04, train loss: 0.0003605, val loss: 0.000577215, \n",
      "time taken for epoch:  9.52741551399231\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "18 0\n",
      "18 10\n",
      "18 20\n",
      "18 30\n",
      "18 40\n",
      "18 50\n",
      "18 60\n",
      "18 70\n",
      "Epoch 18/100, lr = 4.00e-04, train loss: 0.000393722, val loss: 0.000616985, \n",
      "time taken for epoch:  9.960723400115967\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "19 0\n",
      "19 10\n",
      "19 20\n",
      "19 30\n",
      "19 40\n",
      "19 50\n",
      "19 60\n",
      "19 70\n",
      "Epoch 19/100, lr = 4.00e-04, train loss: 0.000293857, val loss: 0.000602341, \n",
      "time taken for epoch:  9.985135555267334\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "20 0\n",
      "20 10\n",
      "20 20\n",
      "20 30\n",
      "20 40\n",
      "20 50\n",
      "20 60\n",
      "20 70\n",
      "Epoch 20/100, lr = 4.00e-04, train loss: 0.000325272, val loss: 0.000811558, \n",
      "time taken for epoch:  10.54055404663086\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "21 0\n",
      "21 10\n",
      "21 20\n",
      "21 30\n",
      "21 40\n",
      "21 50\n",
      "21 60\n",
      "21 70\n",
      "Validation loss decreased (0.000577215 --> 0.000571553). Saving model ...\n",
      "Epoch 21/100, lr = 4.00e-04, train loss: 0.000309951, val loss: 0.000571553, \n",
      "time taken for epoch:  9.111867427825928\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "22 0\n",
      "22 10\n",
      "22 20\n",
      "22 30\n",
      "22 40\n",
      "22 50\n",
      "22 60\n",
      "22 70\n",
      "Epoch 22/100, lr = 4.00e-04, train loss: 0.000314837, val loss: 0.00068726, \n",
      "time taken for epoch:  8.858141899108887\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "23 0\n",
      "23 10\n",
      "23 20\n",
      "23 30\n",
      "23 40\n",
      "23 50\n",
      "23 60\n",
      "23 70\n",
      "Validation loss decreased (0.000571553 --> 0.000553408). Saving model ...\n",
      "Epoch 23/100, lr = 4.00e-04, train loss: 0.000262329, val loss: 0.000553408, \n",
      "time taken for epoch:  9.041303634643555\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "24 0\n",
      "24 10\n",
      "24 20\n",
      "24 30\n",
      "24 40\n",
      "24 50\n",
      "24 60\n",
      "24 70\n",
      "Validation loss decreased (0.000553408 --> 0.000541911). Saving model ...\n",
      "Epoch 24/100, lr = 4.00e-04, train loss: 0.000238672, val loss: 0.000541911, \n",
      "time taken for epoch:  9.32894778251648\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "25 0\n",
      "25 10\n",
      "25 20\n",
      "25 30\n",
      "25 40\n",
      "25 50\n",
      "25 60\n",
      "25 70\n",
      "Validation loss decreased (0.000541911 --> 0.000499532). Saving model ...\n",
      "Epoch 25/100, lr = 4.00e-04, train loss: 0.000270336, val loss: 0.000499532, \n",
      "time taken for epoch:  9.568131446838379\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "26 0\n",
      "26 10\n",
      "26 20\n",
      "26 30\n",
      "26 40\n",
      "26 50\n",
      "26 60\n",
      "26 70\n",
      "Epoch 26/100, lr = 4.00e-04, train loss: 0.000290131, val loss: 0.000550062, \n",
      "time taken for epoch:  8.97998046875\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "27 0\n",
      "27 10\n",
      "27 20\n",
      "27 30\n",
      "27 40\n",
      "27 50\n",
      "27 60\n",
      "27 70\n",
      "Validation loss decreased (0.000499532 --> 0.000476859). Saving model ...\n",
      "Epoch 27/100, lr = 4.00e-04, train loss: 0.0002711, val loss: 0.000476859, \n",
      "time taken for epoch:  9.223663806915283\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "28 0\n",
      "28 10\n",
      "28 20\n",
      "28 30\n",
      "28 40\n",
      "28 50\n",
      "28 60\n",
      "28 70\n",
      "Epoch 28/100, lr = 4.00e-04, train loss: 0.000246338, val loss: 0.000529552, \n",
      "time taken for epoch:  9.283226013183594\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "29 0\n",
      "29 10\n",
      "29 20\n",
      "29 30\n",
      "29 40\n",
      "29 50\n",
      "29 60\n",
      "29 70\n",
      "Epoch 29/100, lr = 4.00e-04, train loss: 0.000247254, val loss: 0.000803263, \n",
      "time taken for epoch:  9.08280873298645\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "30 0\n",
      "30 10\n",
      "30 20\n",
      "30 30\n",
      "30 40\n",
      "30 50\n",
      "30 60\n",
      "30 70\n",
      "Epoch 30/100, lr = 4.00e-04, train loss: 0.000227974, val loss: 0.000540208, \n",
      "time taken for epoch:  9.48883056640625\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "31 0\n",
      "31 10\n",
      "31 20\n",
      "31 30\n",
      "31 40\n",
      "31 50\n",
      "31 60\n",
      "31 70\n",
      "Epoch 31/100, lr = 4.00e-04, train loss: 0.000181246, val loss: 0.000506821, \n",
      "time taken for epoch:  11.03217887878418\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "32 0\n",
      "32 10\n",
      "32 20\n",
      "32 30\n",
      "32 40\n",
      "32 50\n",
      "32 60\n",
      "32 70\n",
      "Epoch 32/100, lr = 4.00e-04, train loss: 0.000174464, val loss: 0.00047881, \n",
      "time taken for epoch:  10.196901321411133\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "33 0\n",
      "33 10\n",
      "33 20\n",
      "33 30\n",
      "33 40\n",
      "33 50\n",
      "33 60\n",
      "33 70\n",
      "Validation loss decreased (0.000476859 --> 0.000418957). Saving model ...\n",
      "Epoch 33/100, lr = 4.00e-04, train loss: 0.000210604, val loss: 0.000418957, \n",
      "time taken for epoch:  10.189561605453491\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "34 0\n",
      "34 10\n",
      "34 20\n",
      "34 30\n",
      "34 40\n",
      "34 50\n",
      "34 60\n",
      "34 70\n",
      "Validation loss decreased (0.000418957 --> 0.00039102). Saving model ...\n",
      "Epoch 34/100, lr = 4.00e-04, train loss: 0.0001734, val loss: 0.00039102, \n",
      "time taken for epoch:  9.74504566192627\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "35 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 10\n",
      "35 20\n",
      "35 30\n",
      "35 40\n",
      "35 50\n",
      "35 60\n",
      "35 70\n",
      "Epoch 35/100, lr = 4.00e-04, train loss: 0.000161352, val loss: 0.000425049, \n",
      "time taken for epoch:  9.615506172180176\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "36 0\n",
      "36 10\n",
      "36 20\n",
      "36 30\n",
      "36 40\n",
      "36 50\n",
      "36 60\n",
      "36 70\n",
      "Epoch 36/100, lr = 4.00e-04, train loss: 0.000162178, val loss: 0.000462768, \n",
      "time taken for epoch:  9.308945655822754\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "37 0\n",
      "37 10\n",
      "37 20\n",
      "37 30\n",
      "37 40\n",
      "37 50\n",
      "37 60\n",
      "37 70\n",
      "Epoch 37/100, lr = 4.00e-04, train loss: 0.000149256, val loss: 0.00043116, \n",
      "time taken for epoch:  9.279958486557007\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "38 0\n",
      "38 10\n",
      "38 20\n",
      "38 30\n",
      "38 40\n",
      "38 50\n",
      "38 60\n",
      "38 70\n",
      "Epoch 38/100, lr = 4.00e-04, train loss: 0.00017493, val loss: 0.000463633, \n",
      "time taken for epoch:  8.83243441581726\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "39 0\n",
      "39 10\n",
      "39 20\n",
      "39 30\n",
      "39 40\n",
      "39 50\n",
      "39 60\n",
      "39 70\n",
      "Epoch 39/100, lr = 4.00e-04, train loss: 0.000156399, val loss: 0.000435705, \n",
      "time taken for epoch:  9.123726606369019\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "40 0\n",
      "40 10\n",
      "40 20\n",
      "40 30\n",
      "40 40\n",
      "40 50\n",
      "40 60\n",
      "40 70\n",
      "Epoch 40/100, lr = 4.00e-04, train loss: 0.000160685, val loss: 0.000447296, \n",
      "time taken for epoch:  9.039623498916626\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "41 0\n",
      "41 10\n",
      "41 20\n",
      "41 30\n",
      "41 40\n",
      "41 50\n",
      "41 60\n",
      "41 70\n",
      "Epoch 41/100, lr = 4.00e-04, train loss: 0.000139579, val loss: 0.000436412, \n",
      "time taken for epoch:  9.446691989898682\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "42 0\n",
      "42 10\n",
      "42 20\n",
      "42 30\n",
      "42 40\n",
      "42 50\n",
      "42 60\n",
      "42 70\n",
      "Epoch 42/100, lr = 4.00e-04, train loss: 0.000140544, val loss: 0.000458227, \n",
      "time taken for epoch:  9.74084997177124\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "43 0\n",
      "43 10\n",
      "43 20\n",
      "43 30\n",
      "43 40\n",
      "43 50\n",
      "43 60\n",
      "43 70\n",
      "Epoch 43/100, lr = 4.00e-04, train loss: 0.000130053, val loss: 0.00041618, \n",
      "time taken for epoch:  10.16659951210022\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "44 0\n",
      "44 10\n",
      "44 20\n",
      "44 30\n",
      "44 40\n",
      "44 50\n",
      "44 60\n",
      "44 70\n",
      "Epoch 44/100, lr = 4.00e-04, train loss: 0.000130931, val loss: 0.000440439, \n",
      "time taken for epoch:  10.566339015960693\n",
      "trained parameters:  8763441\n",
      "----------\n",
      "val loss stagnant for too long, stopping training\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "n_epochs = 100\n",
    "early_stop = 10\n",
    "#unfreeze_epochs = 3\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "val_loss_min = np.Inf\n",
    "stagnant = 0\n",
    "\n",
    "folder_name = \"trainrun_\" + datetime.datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n",
    "os.mkdir(folder_name)\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    \n",
    "    #if epoch == unfreeze_epochs:\n",
    "    #    for param in model.encoder.parameters():\n",
    "    #        param.requires_grad = True        \n",
    "    \n",
    "    e_time = time.time()\n",
    "    # Get value of the current learning rate\n",
    "    current_lr = opt.param_groups[0]['lr']\n",
    "    \n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for bid, (xb, yb) in enumerate(train_dl):\n",
    "        if bid % 10 == 0:\n",
    "            print(epoch, bid)\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb = yb.to(DEVICE)\n",
    "\n",
    "        # forward pass: compute predicted outputs by passing input to the model\n",
    "        output = model(xb)\n",
    "\n",
    "        # calculate the batch losses\n",
    "        loss = loss_func(output, yb)\n",
    "\n",
    "        # clear the gradients of all optimized variables\n",
    "        opt.zero_grad()\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        opt.step()\n",
    "\n",
    "        # Update train loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    \n",
    "    # Validate the model\n",
    "    model.eval() # Activate dropout and BatchNorm in eval mode\n",
    "    with torch.no_grad(): # Save memory bc gradients are not calculated\n",
    "        for xb, yb in val_dl:\n",
    "            xb = xb.to(DEVICE) #(n,1,120,120)\n",
    "            yb = yb.to(DEVICE) #(n,1,120,120)\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing input to the model\n",
    "            output = model(xb) #(n,1,120,120)\n",
    "\n",
    "            # calculate the batch losses\n",
    "            loss = loss_func(output, yb)\n",
    "\n",
    "            # Update validation loss\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    # Calculate average losses of the epoch\n",
    "    train_loss /= len(train_ds)\n",
    "    val_loss /= len(val_ds)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Store best model\n",
    "    if val_loss < val_loss_min:\n",
    "        print(f'Validation loss decreased ({val_loss_min:.6} --> {val_loss:.6}). Saving model ...')\n",
    "\n",
    "        torch.save(model.state_dict(), os.path.join(folder_name, 'ganwoundmodel.pt'))\n",
    "        val_loss_min = val_loss\n",
    "        stagnant = 0\n",
    "    else:\n",
    "        stagnant += 1\n",
    "\n",
    "    # learning rate schedule\n",
    "    lr_scheduler.step(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{n_epochs}, lr = {current_lr:.2e}, \"\n",
    "    f\"train loss: {train_loss:.6}, val loss: {val_loss:.6}, \")\n",
    "    print(\"time taken for epoch: \", time.time() - e_time)\n",
    "    print(\"trained parameters: \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    print(\"-\"*10)\n",
    "    \n",
    "    if stagnant >= early_stop:\n",
    "        print('val loss stagnant for too long, stopping training')\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
